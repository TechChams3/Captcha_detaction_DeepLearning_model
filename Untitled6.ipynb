{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdZ5jfKJNyVkmLqQXbMoM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechChams3/Captcha_detaction_DeepLearning_model/blob/master/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbuCUKgeCm-1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = pd.read_csv('synthetic_amazon_search_volume_data.csv')\n",
        "\n",
        "# Handle missing values and scale features\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "scaler = StandardScaler()\n",
        "data[['BSR', 'price', 'reviews', 'ratings']] = scaler.fit_transform(data[['BSR', 'price', 'reviews', 'ratings']])\n",
        "\n",
        "# Text Processing (for product titles, reviews)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def process_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=50)\n",
        "    return inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YVEukByCoKe",
        "outputId": "3f2196ca-5cae-4cf3-8e43-1c5c09e083b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-129d95a45c52>:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data.fillna(method='ffill', inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Prophet model\n",
        "prophet_df = data[['date', 'search_volume']].rename(columns={'date': 'ds', 'search_volume': 'y'})\n",
        "model = Prophet()\n",
        "model.fit(prophet_df)\n",
        "\n",
        "# Forecast future search volumes\n",
        "future = model.make_future_dataframe(periods=30)\n",
        "forecast = model.predict(future)\n",
        "data['forecast_volume'] = forecast['yhat']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpQC9AtTCqqP",
        "outputId": "07dcce93-5a8f-4345-f9c8-e82bfbd11e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpi3mbkcge/r3vc2i48.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpi3mbkcge/go3_8v05.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=21919', 'data', 'file=/tmp/tmpi3mbkcge/r3vc2i48.json', 'init=/tmp/tmpi3mbkcge/go3_8v05.json', 'output', 'file=/tmp/tmpi3mbkcge/prophet_modelfg15ix5e/prophet_model-20241031101608.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "10:16:08 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "10:16:09 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'date': ['2023-11-26', '2023-11-27', '2023-11-28'],\n",
        "    'feature1': [10, 20, 30],\n",
        "    'target': [15, 25, 35]\n",
        "})\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Extract useful date features\n",
        "data['year'] = data['date'].dt.year\n",
        "data['month'] = data['date'].dt.month\n",
        "data['day'] = data['date'].dt.day\n",
        "\n",
        "# Drop the original 'date' column if itâ€™s not needed\n",
        "data = data.drop(columns=['date'])\n"
      ],
      "metadata": {
        "id": "UnWHIrAtFjTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features like keyword frequency, sentiment analysis (optional)\n",
        "# Add historical values\n",
        "for lag in range(1, 8):\n",
        "    data[f'search_volume_lag_{lag}'] = data['search_volume'].shift(lag)\n"
      ],
      "metadata": {
        "id": "2alGEZRiCtcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "train, test = train_test_split(data.dropna(), test_size=0.2, random_state=42)\n",
        "# Assuming 'target' is your dependent variable (what you want to predict)\n",
        "X_train, y_train = train.drop(['target'], axis=1), train['target']\n",
        "X_test, y_test = test.drop(['target'], axis=1), test['target']\n",
        "\n",
        "\n",
        "# Initialize and train GBM model\n",
        "gbm_model = GradientBoostingRegressor()\n",
        "gbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gbm_model.predict(X_test)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E298mpmnCwvr",
        "outputId": "e2daa49d-f2dc-4901-a799-50ee7c03feff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 10.00013280699444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an LSTM model for deeper sequential learning\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
        "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, _ = self.lstm(input_seq)\n",
        "        predictions = self.linear(lstm_out[-1])\n",
        "        return predictions\n",
        "\n",
        "lstm_model = LSTMModel(input_size=X_train.shape[1], hidden_layer_size=100, output_size=1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    lstm_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = lstm_model(torch.tensor(X_train.values).float())\n",
        "    loss = criterion(outputs, torch.tensor(y_train.values).float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8a_MTfCCzTq",
        "outputId": "1c48a0bc-74c6-4bba-9f5d-527341894683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_pred = (y_pred + lstm_model(torch.tensor(X_test.values).float()).detach().numpy().flatten()) / 2\n",
        "print(\"Ensemble MAE:\", mean_absolute_error(y_test, ensemble_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jLBdy52C1w4",
        "outputId": "ef66203b-b403-4130-f84a-3492f26fd83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble MAE: 1.2943351229538909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz6PuBdQC4aO",
        "outputId": "f98b656e-1be3-4505-83d3-323a4df0ab21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['feature1', 'target', 'year', 'month', 'day'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample keyword list\n",
        "test_keywords = [\n",
        "    \"wireless earbuds\",\n",
        "    \"running shoes\",\n",
        "    \"coffee maker\",\n",
        "    \"gaming mouse\",\n",
        "    \"office chair\"\n",
        "]\n",
        "\n",
        "# Sample features for testing\n",
        "# Assuming we want to test for a specific date (e.g., November 1, 2024)\n",
        "current_year = 2024\n",
        "current_month = 11\n",
        "current_day = 1\n",
        "\n",
        "# Create a DataFrame for testing\n",
        "# Modify 'feature1' based on your actual feature values\n",
        "test_data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5],  # Placeholder values for feature1\n",
        "    'target': [0, 0, 0, 0, 0],    # Placeholder for target; this will be ignored in predictions\n",
        "    'year': [current_year] * len(test_keywords),\n",
        "    'month': [current_month] * len(test_keywords),\n",
        "    'day': [current_day] * len(test_keywords)\n",
        "})\n",
        "\n",
        "# Add keywords as a new column (if necessary)\n",
        "test_data['keyword'] = test_keywords\n",
        "\n",
        "# Select the features to predict\n",
        "X_test = test_data.drop(columns=['target', 'keyword'])\n",
        "\n",
        "# Make predictions using your trained model (assuming gbm_model is defined)\n",
        "predictions = gbm_model.predict(X_test)\n",
        "\n",
        "# Display the predictions\n",
        "for keyword, prediction in zip(test_keywords, predictions):\n",
        "    print(f\"Keyword: {keyword}, Predicted Search Volume: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcPwzSwWGasw",
        "outputId": "86324a23-f388-4e5c-c120-298e876b691d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyword: wireless earbuds, Predicted Search Volume: 25.00013280699444\n",
            "Keyword: running shoes, Predicted Search Volume: 25.00013280699444\n",
            "Keyword: coffee maker, Predicted Search Volume: 25.00013280699444\n",
            "Keyword: gaming mouse, Predicted Search Volume: 25.00013280699444\n",
            "Keyword: office chair, Predicted Search Volume: 25.00013280699444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxLpbgRUItB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}